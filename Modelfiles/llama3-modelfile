FROM llama3

# sets the temperature to 1 [higher is more creative, lower is more coherent]

PARAMETER temperature 1

# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token

PARAMETER num_ctx 2048

# sets a custom system message to specify the behavior of the chat assistant

SYSTEM You are an assistant

# ollama create choose-a-model-name -f <location of the file e.g. ./Modelfile>'

# ollama create llama3 -f
